{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7jv_gPH7mO6"
   },
   "source": [
    "# Turkish Legal Question-Answering System with LoRA Fine-tuning\n",
    "\n",
    "This notebook demonstrates fine-tuning Trendyol LLM 7B on Turkish legal Q&A dataset using QLoRA (4-bit quantization + LoRA).\n",
    "\n",
    "## Project Overview\n",
    "- **Base Model**: Trendyol/Trendyol-LLM-7b-chat-v0.1\n",
    "- **Dataset**: turkish-law-chatbot (14.9K Q&A pairs)\n",
    "- **Method**: QLoRA (4-bit quantization + LoRA adapters)\n",
    "- **Hardware**: 8GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CUDA memory allocation optimization\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U1YDJa97mO_"
   },
   "source": [
    "## 1. Load Model with 4-bit Quantization\n",
    "\n",
    "**Quantization:** Reduces model weights from 32-bit to 4-bit\n",
    "- Memory: Trendyol LLM 7B → ~4GB (4-bit)\n",
    "- Uses NF4 (Normal Float 4-bit) for better accuracy\n",
    "- Compute dtype: bfloat16 for stable training\n",
    "- **Optimized for Turkish language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ABwkAEER7mPB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7c896481854f52bc903e3cd4fd5bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully\n",
      "Memory footprint: 3.69 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Model name - Turkish optimized model by Trendyol\n",
    "model_id = \"Trendyol/Trendyol-LLM-7b-chat-v0.1\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(f\"✓ Model loaded successfully\")\n",
    "print(f\"Memory footprint: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDaz43Yo7mPB"
   },
   "source": [
    "## 2. Test Base Model\n",
    "\n",
    "Test the base model before fine-tuning to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wvGZuKK97mPC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trafik cezalarına itiraz süreci, trafik cezası aldığınıza dair tebligatınızın gelmesiyle başlar.Express'te yer alan bilgilere göre, trafik cezasına itiraz etmek için öncelikle cezanın size tebliğ edildiği tarihten itibaren 15 gün içinde itiraz etmeniz gerekiyor. Bu süre içinde itiraz etmezseniz, ceza kesinleşiyor ve ödeme yapmanız gerekiyor. İtiraz etmek için trafik cezası aldığınız tarihten itibaren 15 gün içinde itiraz etmeniz gerekiyor. İtiraz etmek için trafik cezasının size tebliğ edildiği tarihten itibaren 15 gün içinde itiraz etmeniz gerekiyor. İtiraz etmek için trafik cezasının size tebliğ edildiği tarihten itibaren 15 gün içinde itiraz etmeniz gerekiyor. İtiraz etmek için trafik cezasının size tebliğ edildiği tarihten itibaren 15 gün içinde itiraz etmeniz gerekiyor. İtiraz etmek için trafik cezasının size tebliğ edildiği tarihten itibaren 15 gün içinde itiraz etmeniz gerekiyor. İtiraz etmek için trafik cezasının size tebliğ edildiği tarihten itibaren 15 gün içinde itiraz etmeniz gerekiyor. İtiraz etmek için trafik cezasının size tebliğ edildiği tarihten itibaren 15 gün içinde itiraz etmeniz gerekiyor. İtiraz etmek için trafik cezasının size tebliğ edildiği tarihten itibaren 15 gün içinde itiraz etmeniz gerekiyor. İtiraz etmek için trafik cezasının size tebliğ edildiği tarihten itibaren 15 gün içinde itiraz etmeniz gerekiyor. İtiraz etmek için trafik cezasının size tebliğ edildiği tarihten itibaren 15 gün içinde itiraz etmeniz gerekiyor.\n"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Trafik cezalarına itiraz süreci nasıl işler?\"}\n",
    "]\n",
    "\n",
    "# Tokenize and generate\n",
    "text = tokenizer.apply_chat_template(messages, \n",
    "                                     tokenize=False, \n",
    "                                     add_generation_prompt=True)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0hRUsnB7mPD"
   },
   "source": [
    "## 3. Load Dataset\n",
    "\n",
    "Turkish legal Q&A dataset with ~14.9K question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aZPjVq-87mPE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 13354\n",
      "Test examples: 1500\n",
      "\n",
      "Columns: ['Soru', 'Cevap']\n",
      "\n",
      "First example:\n",
      "{'Soru': \"Anayasa madde 1'e göre, türkiye'nin devlet şekli nedir\", 'Cevap': \"Anayasa madde 1'e göre, türkiye'nin devlet şekli cumhuriyettir. bu madde, türkiye'nin yönetim biçiminin halkın egemenliğine dayandığını ve bu yönetim biçiminin cumhuriyet olduğunu belirler. cumhuriyet, halkın kendi kendini yönetme biçimi olarak kabul edilir ve türkiye cumhuriyeti'nin temel yönetim ilkesi olarak anayasal güvence altına alınmıştır.\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"Renicames/turkish-law-chatbot\")\n",
    "\n",
    "print(f\"Train examples: {len(dataset['train'])}\")\n",
    "print(f\"Test examples: {len(dataset['test'])}\")\n",
    "print(f\"\\nColumns: {dataset['train'].column_names}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFo7rQ9t7mPF"
   },
   "source": [
    "## 4. Define Formatting Function\n",
    "\n",
    "Converts Q&A pairs into chat format that the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "apCpPyuE7mPG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted example:\n",
      "<s>[INST] Anayasa madde 1'e göre, türkiye'nin devlet şekli nedir [/INST] Anayasa madde 1'e göre, türkiye'nin devlet şekli cumhuriyettir. bu madde, türkiye'nin yönetim biçiminin halkın egemenliğine dayandığını ve bu yönetim biçiminin cumhuriyet olduğunu belirler. cumhuriyet, halkın kendi kendini yöne ...\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    \"\"\"\n",
    "    Converts a single Q&A example to chat format.\n",
    "\n",
    "    Args:\n",
    "        example: Dict with 'Soru' (question) and 'Cevap' (answer) keys\n",
    "\n",
    "    Returns:\n",
    "        Formatted text string in chat template format\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example['Soru']},\n",
    "        {\"role\": \"assistant\", \"content\": example['Cevap']}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template (adds special tokens)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False  # Include answer for training\n",
    "    )\n",
    "\n",
    "    return text\n",
    "\n",
    "# Test formatting function\n",
    "formatted_example = formatting_func(dataset['train'][0])\n",
    "print(\"Formatted example:\")\n",
    "print(formatted_example[:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqWGMLPw7mPH"
   },
   "source": [
    "## 5. Configure LoRA\n",
    "\n",
    "**LoRA (Low-Rank Adaptation):**\n",
    "- Trains only small adapter matrices instead of full model\n",
    "- 0.24% trainable parameters\n",
    "- Memory efficient and fast\n",
    "\n",
    "**Parameters:**\n",
    "- `r=16`: Rank of adapter matrices\n",
    "- `alpha=32`: Controls how strongly LoRA adapters influence the frozen model (higher = stronger adapter effect)\n",
    "- Target modules: Attention layers (q, k, v, o projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9KDho5hA7mPI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LoRA configured\n",
      "trainable params: 16,777,216 || all params: 6,855,315,456 || trainable%: 0.2447\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Prepare quantized model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Enable gradient checkpointing (saves memory)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Add LoRA adapters to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"✓ LoRA configured\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE1N4qrv7mPJ"
   },
   "source": [
    "## 6. Configure Training Arguments\n",
    "\n",
    "**Key Parameters:**\n",
    "- `num_train_epochs=2`: Train for 2 complete passes through the dataset\n",
    "- `batch_size=2`: Process 2 examples at once\n",
    "- `gradient_accumulation_steps=4`: Effective batch size = 8\n",
    "- `learning_rate=2e-4`: Standard for LoRA\n",
    "- `bf16=True`: Use bfloat16 for memory efficiency\n",
    "- `save_strategy=\"epoch\"`: Save checkpoint after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "os4vF0Pp7mPJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training arguments configured\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trendyol-turkish-law-lora\",        # Output directory for checkpoints\n",
    "    num_train_epochs=2,                              # 2 epochs\n",
    "    per_device_train_batch_size=2,                   # Batch size per device\n",
    "    gradient_accumulation_steps=4,                   # Effective batch = 8\n",
    "    learning_rate=2e-4,                              # Learning rate\n",
    "    logging_steps=100,                               # Log every 100 steps\n",
    "    save_strategy=\"epoch\",                           # Save by epochs\n",
    "    bf16=True,                                       # Use bfloat16 for training\n",
    "    optim=\"paged_adamw_8bit\",                        # 8-bit optimizer for memory efficiency\n",
    ")\n",
    "\n",
    "print(\"✓ Training arguments configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IaattCo7mPJ"
   },
   "source": [
    "## 7. Create Trainer\n",
    "\n",
    "SFTTrainer (Supervised Fine-Tuning Trainer) handles the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6SP0jEtS7mPK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer created and ready\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    formatting_func=formatting_func\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer created and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbwblOiR7mPL"
   },
   "source": [
    "## 8. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g5WTQ8Af7mPL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3340' max='3340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3340/3340 4:41:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.981600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.878100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.914900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.853500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.872500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.793300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.793500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.813100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.659800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.640900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.667700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.647700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.641400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.636400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.604100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.597600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.632600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.622500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3340, training_loss=0.8177105366826771, metrics={'train_runtime': 16875.1108, 'train_samples_per_second': 1.583, 'train_steps_per_second': 0.198, 'total_flos': 9.926673952815514e+16, 'train_loss': 0.8177105366826771, 'entropy': 0.6569879136647389, 'num_tokens': 1993798.0, 'mean_token_accuracy': 0.8258876568952184, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJiHjmEK7mPL"
   },
   "source": [
    "## 9. Save Model\n",
    "\n",
    "After training completes, save the LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cf6TEhLe7mPL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"./trendyol-turkish-law-lora-final\")\n",
    "tokenizer.save_pretrained(\"./trendyol-turkish-law-lora-final\")\n",
    "print(\"✓ Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k50r9SIO7mPM"
   },
   "source": [
    "## 10. Evaluation\n",
    "\n",
    "Test the fine-tuned model and compare with base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XTV1iVsu7mPM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1aca1420a74837878fb5a78980ded8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters...\n",
      "✓ Fine-tuned model loaded successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Model paths\n",
    "base_model_name = \"Trendyol/Trendyol-LLM-7b-chat-v0.1\"\n",
    "adapter_path = \"./trendyol-turkish-law-lora-final\"\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Loading LoRA adapters...\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "print(\"✓ Fine-tuned model loaded successfully\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Single Example Test\n",
    "\n",
    "Test a single question to see model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Trafik cezalarına itiraz süreci nasıl işler?\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Fine-tuned model response:\n",
      "Trafik cezalarına itiraz süreci, cezanın tebliğinden itibaren 15 gün içinde sulh ceza hakimliğine başvurarak yapılabilir. \n"
     ]
    }
   ],
   "source": [
    "# Test question\n",
    "test_question = \"Trafik cezalarına itiraz süreci nasıl işler?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": test_question}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "print(\"Generating response...\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"\\nFine-tuned model response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Test Set Evaluation\n",
    "\n",
    "Evaluate model performance on the full test set (1500 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 1500 test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [09:42<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Test Loss: 0.7859\n",
      "Perplexity: 2.1945\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load test data\n",
    "test_set = load_dataset(\"Renicames/turkish-law-chatbot\", split=\"test\")\n",
    "\n",
    "print(f\"Evaluating on {len(test_set)} test examples...\")\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "num_samples = 0\n",
    "\n",
    "for example in tqdm(test_set):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example['Soru']},\n",
    "        {\"role\": \"assistant\", \"content\": example['Cevap']}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs.input_ids)\n",
    "        total_loss += outputs.loss.item()\n",
    "        num_samples += 1\n",
    "\n",
    "avg_loss = total_loss / num_samples\n",
    "perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Training Results\n",
    "- **Training Loss**: 0.818 (final)\n",
    "- **Training Time**: ~4.7 hours (3340 steps)\n",
    "- **Trainable Parameters**: 16.7M (0.24% of total)\n",
    "\n",
    "### Evaluation Results\n",
    "- **Test Loss**: 0.7859\n",
    "- **Perplexity**: 2.19 (lower is better)\n",
    "- **Model Size**: 33MB (LoRA adapters only)\n",
    "\n",
    "### Key Achievements\n",
    "- ✅ Successfully fine-tuned 7B model on 8GB VRAM\n",
    "- ✅ Model learned Turkish legal terminology\n",
    "- ✅ Improved response quality (no more repetition loops)\n",
    "- ✅ Low perplexity indicates good language modeling\n",
    "\n",
    "### Next Steps\n",
    "- Deploy model as API or chatbot\n",
    "- Fine-tune further on domain-specific data\n",
    "- Experiment with different LoRA ranks\n",
    "- Test on edge cases and complex legal queries"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "turkish-legal-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
